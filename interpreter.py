import os
import time
import queue
import threading
import sys
import numpy as np
import pyaudiowpatch as pa
from scipy import signal
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator

# ==================================================================
# ðŸ”§ ì‹œìŠ¤í…œ ì„¤ì •
# ==================================================================
MODEL_SIZE = "tiny.en"     # ì†ë„ ìµœì í™”
TARGET_SR = 16000
VAD_THRESHOLD = 0.015      # ìž¡ìŒ ë¬´ì‹œ ìž„ê³„ê°’
UPDATE_INTERVAL = 0.3      # 0.3ì´ˆë§ˆë‹¤ í™”ë©´ ê°±ì‹  (ë°˜ì‘ì†ë„ UP)
SILENCE_TIMEOUT = 0.8      # 0.8ì´ˆ ì¡°ìš©í•˜ë©´ ë¬¸ìž¥ í™•ì •
# ==================================================================

# ìœˆë„ìš°/ë§¥ì— ë”°ë¥¸ í™”ë©´ ì§€ìš°ê¸° ëª…ë ¹ì–´ ì„¤ì •
CLEAR_CMD = 'cls' if os.name == 'nt' else 'clear'

print(f"ðŸ”„ ì‹œìŠ¤í…œ ìž¬êµ¬ì¶• ì¤‘... (Model: {MODEL_SIZE})")

try:
    # CPU ì½”ì–´ 4ê°œ ì‚¬ìš©
    model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8", cpu_threads=4)
    translator = GoogleTranslator(source='auto', target='ko')
except Exception as e:
    print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    os._exit(1)

audio_queue = queue.Queue()
stop_event = threading.Event()

def get_loopback_device(p):
    try:
        wasapi = p.get_host_api_info_by_type(pa.paWASAPI)
        default_out = p.get_device_info_by_index(wasapi["defaultOutputDevice"])
        for i in range(p.get_device_count()):
            dev = p.get_device_info_by_index(i)
            if dev["hostApi"] == wasapi["index"] and dev["maxInputChannels"] > 0:
                if dev["name"] == default_out["name"] or "Loopback" in dev["name"]:
                    return dev
    except: pass
    return None

def process_worker():
    """í™”ë©´ì„ ì§€ìš°ê³  ë‹¤ì‹œ ê·¸ë¦¬ëŠ” ë°©ì‹ (ì¤‘ë³µ ì›ì²œ ì°¨ë‹¨)"""
    accumulated_audio = []
    last_transcribe_time = time.time()
    
    # [í•µì‹¬] í™•ì •ëœ ë¬¸ìž¥ë“¤ì„ ì €ìž¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ìµœê·¼ 3ê°œë§Œ ë³´ì—¬ì¤Œ)
    history = []
    current_sentence = ""
    current_translation = ""

    while not stop_event.is_set():
        try:
            try:
                item = audio_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            # ë¬¸ìž¥ ì¢…ë£Œ ì‹ í˜¸
            if item is None:
                if current_sentence:
                    # í˜„ìž¬ ë¬¸ìž¥ì„ ì—­ì‚¬(History)ì— ê¸°ë¡í•˜ê³  ë²„í¼ ë¹„ì›€
                    history.append(f"ðŸ‡ºðŸ‡¸ {current_sentence}\nðŸ‡°ðŸ‡· {current_translation}")
                    if len(history) > 3: # í™”ë©´ ê½‰ ì°¨ë‹ˆê¹Œ ìµœê·¼ 3ê°œë§Œ ìœ ì§€
                        history.pop(0)
                    
                    current_sentence = ""
                    current_translation = ""
                    accumulated_audio = []
                    
                    # í™”ë©´ ê°±ì‹ 
                    os.system(CLEAR_CMD)
                    print("\n".join(history))
                    print(f"\nðŸŽ§ ë“£ëŠ” ì¤‘... (ëŒ€ê¸°)")
                continue

            accumulated_audio.append(item)

            # ì‹¤ì‹œê°„ ë¶„ì„ (0.3ì´ˆë§ˆë‹¤)
            if time.time() - last_transcribe_time > UPDATE_INTERVAL:
                full_audio = np.concatenate(accumulated_audio)
                
                # Whisper ì¸ì‹
                segments, _ = model.transcribe(
                    full_audio,
                    beam_size=1,
                    language="en",
                    condition_on_previous_text=False
                )
                
                text = " ".join([seg.text for seg in segments]).strip()
                
                # ë‚´ìš©ì´ ìžˆê³ , ì´ì „ê³¼ ë‹¤ë¥¼ ë•Œë§Œ ê°±ì‹ 
                if len(text) > 1 and text != current_sentence:
                    try:
                        kor = translator.translate(text)
                        
                        current_sentence = text
                        current_translation = kor
                        
                        # [í•µì‹¬ ê¸°ìˆ ] í™”ë©´ ì „ì²´ë¥¼ ì§€ìš°ê³ (CLS) ë‹¤ì‹œ ì”€
                        # ì´ë ‡ê²Œ í•˜ë©´ ê¸€ìžê°€ ê²¹ì¹˜ê±°ë‚˜ ë°˜ë³µë  ì¼ì´ 0%
                        os.system(CLEAR_CMD)
                        
                        # 1. ì§€ë‚˜ê°„ ëŒ€í™” ë³´ì—¬ì£¼ê¸°
                        if history:
                            print("\n".join(history))
                            print("-" * 30)
                        
                        # 2. í˜„ìž¬ ë§í•˜ê³  ìžˆëŠ” ë¬¸ìž¥ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
                        print(f"â–¶ {current_sentence}")
                        print(f"â–· {current_translation}")
                        
                    except: pass
                
                last_transcribe_time = time.time()

        except Exception:
            pass

def main():
    p = pa.PyAudio()
    try:
        target = get_loopback_device(p)
        if not target:
            print("âŒ ìž¥ì¹˜ ì—†ìŒ")
            return

        native_rate = int(target["defaultSampleRate"])
        input_channels = target["maxInputChannels"]
        
        os.system(CLEAR_CMD)
        print(f"âœ… ì—°ê²°ë¨: {target['name']}")
        print("ðŸš€ [ë¼ì´ë¸Œ ìº¡ì…˜ V2] í™”ë©´ ë¦¬í”„ë ˆì‹œ ëª¨ë“œ")
        print("   (ì¤‘ë³µëœ ê¸€ìžê°€ ì ˆëŒ€ ìŒ“ì´ì§€ ì•ŠìŠµë‹ˆë‹¤)")
        time.sleep(2)

        stream = p.open(format=pa.paFloat32,
                        channels=input_channels,
                        rate=native_rate,
                        input=True,
                        input_device_index=target["index"])

        t = threading.Thread(target=process_worker)
        t.daemon = True
        t.start()

        is_speaking = False
        silence_start = None
        
        while True:
            try:
                chunk_len = int(native_rate * 0.1)
                raw_data = stream.read(chunk_len, exception_on_overflow=False)
                audio_float = np.frombuffer(raw_data, dtype=np.float32)
                
                if input_channels > 1:
                    audio_mono = audio_float.reshape(-1, input_channels).mean(axis=1)
                else:
                    audio_mono = audio_float
                
                num_samples = int(len(audio_mono) * TARGET_SR / native_rate)
                resampled_chunk = signal.resample(audio_mono, num_samples)
                
                rms = np.sqrt(np.mean(resampled_chunk**2))
                
                # VAD ë¡œì§
                if rms > VAD_THRESHOLD:
                    is_speaking = True
                    silence_start = None
                    audio_queue.put(resampled_chunk)
                else:
                    if is_speaking:
                        if silence_start is None:
                            silence_start = time.time()
                        
                        audio_queue.put(resampled_chunk)
                        
                        if time.time() - silence_start > SILENCE_TIMEOUT:
                            is_speaking = False
                            silence_start = None
                            audio_queue.put(None) # ë¬¸ìž¥ í™•ì • ì‹ í˜¸
                    else:
                        pass

            except IOError: continue
            except KeyboardInterrupt: break

    except KeyboardInterrupt: print("\nì¢…ë£Œ")
    finally:
        stop_event.set()
        if 'stream' in locals(): stream.stop_stream(); stream.close()
        p.terminate()

if __name__ == "__main__":
    main()