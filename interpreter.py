import os
import re
import time
import queue
import threading
import sys
import collections
import numpy as np
import pyaudiowpatch as pa
from scipy import signal
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator

# ==================================================================
# ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •
# ==================================================================
MODEL_SIZE = "tiny.en"     # ì†ë„ ìµœì í™”
TARGET_SR = 16000
VAD_THRESHOLD = 0.015      # ì¡ìŒ ë¬´ì‹œ ì„ê³„ê°’
UPDATE_INTERVAL = 0.3      # 0.3ì´ˆë§ˆë‹¤ í™”ë©´ ê°±ì‹  (ë°˜ì‘ì†ë„ UP)
SILENCE_TIMEOUT = 0.6      # 0.6ì´ˆ ì¡°ìš©í•˜ë©´ ë¬¸ì¥ í™•ì • (0.8 â†’ 0.6ìœ¼ë¡œ ë‹¨ì¶•)
PREROLL_CHUNKS = 3         # í”„ë¦¬ë¡¤ ë²„í¼ í¬ê¸° (0.3ì´ˆ = 3 chunks Ã— 0.1ì´ˆ)
ASYNC_TRANSLATION_MIN_CHANGE = 10  # ë¹„ë™ê¸° ë²ˆì—­ íŠ¸ë¦¬ê±° ìµœì†Œ ê¸€ì ë³€í™”ëŸ‰
# ==================================================================

# ë¬¸ì¥ ì¢…ë£Œ ë¶€í˜¸ ê°ì§€ (ìˆ«ì ì† ë§ˆì¹¨í‘œ, ì¤„ì„í‘œ ì œì™¸)
SENTENCE_END_PATTERN = re.compile(r'(?<!\d)(?<!\.)([.!?])(?=\s|$)')


def split_sentences(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì™„ì„±ëœ ë¬¸ì¥ê³¼ ë¯¸ì™„ì„± ë‚˜ë¨¸ì§€ë¥¼ ë¶„ë¦¬"""
    matches = list(SENTENCE_END_PATTERN.finditer(text))
    if not matches:
        return [], text  # ì™„ì„±ëœ ë¬¸ì¥ ì—†ìŒ

    last_match = matches[-1]
    split_pos = last_match.end()

    completed = text[:split_pos].strip()
    remaining = text[split_pos:].strip()

    return [completed], remaining

# â”€â”€ Step 4: Windows ANSI VT100 í™œì„±í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if os.name == 'nt':
    import ctypes
    _STD_OUTPUT_HANDLE = -11
    _ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004
    kernel32 = ctypes.windll.kernel32
    _handle = kernel32.GetStdHandle(_STD_OUTPUT_HANDLE)
    _mode = ctypes.c_ulong(0)
    kernel32.GetConsoleMode(_handle, ctypes.byref(_mode))
    kernel32.SetConsoleMode(_handle, _mode.value | _ENABLE_VIRTUAL_TERMINAL_PROCESSING)

def _ansi_clear_screen():
    """í™”ë©´ ì „ì²´ë¥¼ ANSI ì»¤ì„œ ì œì–´ë¡œ ì§€ì›ë‹ˆë‹¤ (ê¹œë¹¡ì„ ì—†ìŒ)."""
    sys.stdout.write('\033[H\033[J')
    sys.stdout.flush()

def _ansi_redraw(history, live_text, live_translation, is_partial):
    """ì»¤ì„œë¥¼ í™ˆìœ¼ë¡œ ì´ë™í•œ ë’¤ ì „ì²´ í™”ë©´ì„ ë‹¤ì‹œ ê·¸ë¦½ë‹ˆë‹¤."""
    sys.stdout.write('\033[H\033[J')
    if history:
        sys.stdout.write("\n".join(history) + "\n")
        sys.stdout.write("-" * 30 + "\n")
    if live_text:
        if is_partial:
            # íšŒìƒ‰ìœ¼ë¡œ í‘œì‹œ (partial)
            sys.stdout.write(f"\033[90mâ–¶ {live_text}\033[0m\n")
            sys.stdout.write(f"\033[90mâ–· {live_translation}\033[0m\n")
        else:
            # í°ìƒ‰ êµµê²Œ í‘œì‹œ (final)
            sys.stdout.write(f"\033[1mâ–¶ {live_text}\033[0m\n")
            sys.stdout.write(f"\033[1mâ–· {live_translation}\033[0m\n")
    else:
        sys.stdout.write("\nğŸ§ ë“£ëŠ” ì¤‘... (ëŒ€ê¸°)\n")
    sys.stdout.flush()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print(f"ğŸ”„ ì‹œìŠ¤í…œ ì¬êµ¬ì¶• ì¤‘... (Model: {MODEL_SIZE})")

try:
    # CPU ì½”ì–´ 4ê°œ ì‚¬ìš©
    model = WhisperModel(MODEL_SIZE, device="cpu", compute_type="int8", cpu_threads=4)
    translator = GoogleTranslator(source='auto', target='ko')
except Exception as e:
    print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    os._exit(1)

audio_queue = queue.Queue()
stop_event = threading.Event()

# â”€â”€ Step 2: AsyncTranslator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AsyncTranslator:
    """ë¶€ë¶„ ì¸ì‹ ì¤‘ì—ëŠ” ë¹„ë™ê¸°ë¡œ, ë¬¸ì¥ í™•ì • ì‹œì—ëŠ” ë™ê¸°ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    def __init__(self, translator):
        self.translator = translator
        self.last_translated = ""
        self.last_source = ""
        self.lock = threading.Lock()
        self._running = False

    def request(self, text, is_final=False):
        if is_final:
            try:
                result = self.translator.translate(text)
                with self.lock:
                    self.last_translated = result
                    self.last_source = text
            except Exception: pass
            return

        with self.lock:
            if abs(len(text) - len(self.last_source)) < ASYNC_TRANSLATION_MIN_CHANGE:
                return
            if self._running:
                return
            self._running = True

        def _do_translate():
            try:
                result = self.translator.translate(text)
                with self.lock:
                    self.last_translated = result
                    self.last_source = text
            except Exception: pass
            finally:
                with self.lock:
                    self._running = False

        threading.Thread(target=_do_translate, daemon=True).start()

    @property
    def current(self):
        with self.lock:
            return self.last_translated
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ Step 1: stabilize_text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stabilize_text(previous: str, current: str, max_rewrite_words: int = 8) -> str:
    """ê³µí†µ ì ‘ë‘(LCP)ë¥¼ ê³ ì •í•˜ê³  ë³€ê²½ ë²”ìœ„ë¥¼ ë§ˆì§€ë§‰ Në‹¨ì–´ë¡œ ì œí•œí•©ë‹ˆë‹¤."""
    prev_words = previous.split()
    curr_words = current.split()

    common_len = 0
    for p, c in zip(prev_words, curr_words):
        if p == c:
            common_len += 1
        else:
            break

    locked_part = prev_words[:common_len]
    new_tail = curr_words[common_len:]

    if len(new_tail) > max_rewrite_words:
        new_tail = new_tail[-max_rewrite_words:]

    return " ".join(locked_part + new_tail)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_loopback_device(p):
    try:
        wasapi = p.get_host_api_info_by_type(pa.paWASAPI)
        default_out = p.get_device_info_by_index(wasapi["defaultOutputDevice"])
        for i in range(p.get_device_count()):
            dev = p.get_device_info_by_index(i)
            if dev["hostApi"] == wasapi["index"] and dev["maxInputChannels"] > 0:
                if dev["name"] == default_out["name"] or "Loopback" in dev["name"]:
                    return dev
    except: pass
    return None

def process_worker():
    """Live/Final 2íŠ¸ë™ + prefix-lock + ë¹„ë™ê¸° ë²ˆì—­ + ANSI UI"""
    accumulated_audio = []
    last_transcribe_time = time.time()

    # [í•µì‹¬] í™•ì •ëœ ë¬¸ì¥ë“¤ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ìµœê·¼ 3ê°œë§Œ ë³´ì—¬ì¤Œ)
    history = []
    current_sentence = ""
    async_translator = AsyncTranslator(translator)

    while not stop_event.is_set():
        try:
            try:
                item = audio_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            # â”€â”€ Step 5: Final Track (ë¬¸ì¥ ì¢…ë£Œ ì‹ í˜¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if item is None:
                if accumulated_audio:
                    full_audio = np.concatenate(accumulated_audio)
                    # Final: beam_size=3ìœ¼ë¡œ ì •í™•í•œ ì¬ì „ì‚¬
                    segments, _ = model.transcribe(
                        full_audio,
                        beam_size=3,
                        language="en",
                        vad_filter=True,
                        vad_parameters=dict(min_silence_duration_ms=500),
                        condition_on_previous_text=False
                    )
                    final_text = " ".join([seg.text for seg in segments]).strip()

                    if final_text:
                        # ë™ê¸° ë²ˆì—­ (final)
                        async_translator.request(final_text, is_final=True)
                        final_translation = async_translator.current

                        history.append(f"ğŸ‡ºğŸ‡¸ {final_text}\nğŸ‡°ğŸ‡· {final_translation}")
                        if len(history) > 3:
                            history.pop(0)

                current_sentence = ""
                accumulated_audio = []
                _ansi_redraw(history, "", "", False)
                continue
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            accumulated_audio.append(item)

            # â”€â”€ Step 5: Live Track (0.3ì´ˆë§ˆë‹¤) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if time.time() - last_transcribe_time > UPDATE_INTERVAL:
                full_audio = np.concatenate(accumulated_audio)

                # Live: beam_size=1, vad_filter=True â€” ë¹ ë¥¸ ë¶€ë¶„ ìë§‰
                segments, _ = model.transcribe(
                    full_audio,
                    beam_size=1,
                    language="en",
                    vad_filter=True,
                    vad_parameters=dict(min_silence_duration_ms=500),
                    condition_on_previous_text=False
                )

                raw_text = " ".join([seg.text for seg in segments]).strip()

                if len(raw_text) > 1:
                    # Step 1: prefix-lock ì•ˆì •í™”
                    stable_text = stabilize_text(current_sentence, raw_text)

                    # ë¬¸ì¥ ì¢…ë£Œ ë¶€í˜¸ ê°ì§€ â†’ ì¦‰ì‹œ í™•ì •
                    completed_sentences, remaining = split_sentences(stable_text)

                    if completed_sentences:
                        for sentence in completed_sentences:
                            async_translator.request(sentence, is_final=True)
                            translation = async_translator.current
                            history.append(f"ğŸ‡ºğŸ‡¸ {sentence}\nğŸ‡°ğŸ‡· {translation}")
                            if len(history) > 3:
                                history.pop(0)

                        current_sentence = remaining

                        # ì˜¤ë””ì˜¤ ë²„í¼ ì •ë¦¬: ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ë¹„ìœ¨ë§Œí¼ë§Œ ìœ ì§€
                        if remaining and stable_text:
                            ratio = len(remaining) / len(stable_text)
                            total_samples = sum(len(a) for a in accumulated_audio)
                            keep_samples = int(total_samples * ratio)
                            full_audio = np.concatenate(accumulated_audio)
                            accumulated_audio = [full_audio[-keep_samples:]] if keep_samples > 0 else []
                        else:
                            accumulated_audio = []

                        _ansi_redraw(history, remaining, async_translator.current, is_partial=bool(remaining))
                    else:
                        current_sentence = stable_text

                        # Step 2: ë¹„ë™ê¸° ë²ˆì—­ ìš”ì²­ (partial)
                        async_translator.request(stable_text, is_final=False)

                        _ansi_redraw(history, stable_text, async_translator.current, is_partial=True)

                last_transcribe_time = time.time()
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        except Exception:
            pass

def main():
    p = pa.PyAudio()
    try:
        target = get_loopback_device(p)
        if not target:
            print("âŒ ì¥ì¹˜ ì—†ìŒ")
            return

        native_rate = int(target["defaultSampleRate"])
        input_channels = target["maxInputChannels"]

        _ansi_clear_screen()
        print(f"âœ… ì—°ê²°ë¨: {target['name']}")
        print("ğŸš€ [ë¼ì´ë¸Œ ìº¡ì…˜ V3] ë¼ì´ë¸Œìº¡ì…˜ ìˆ˜ì¤€ ì—…ê·¸ë ˆì´ë“œ")
        print("   Prefix-lock | ë¹„ë™ê¸° ë²ˆì—­ | VAD í”„ë¦¬ë¡¤ | ANSI UI")
        time.sleep(2)

        stream = p.open(format=pa.paFloat32,
                        channels=input_channels,
                        rate=native_rate,
                        input=True,
                        input_device_index=target["index"])

        t = threading.Thread(target=process_worker)
        t.daemon = True
        t.start()

        is_speaking = False
        silence_start = None
        # â”€â”€ Step 3: í”„ë¦¬ë¡¤ ë²„í¼ (ìµœê·¼ 0.3ì´ˆ = 3 chunks) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        preroll_buf = collections.deque(maxlen=PREROLL_CHUNKS)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        while True:
            try:
                chunk_len = int(native_rate * 0.1)
                raw_data = stream.read(chunk_len, exception_on_overflow=False)
                audio_float = np.frombuffer(raw_data, dtype=np.float32)

                if input_channels > 1:
                    audio_mono = audio_float.reshape(-1, input_channels).mean(axis=1)
                else:
                    audio_mono = audio_float

                num_samples = int(len(audio_mono) * TARGET_SR / native_rate)
                resampled_chunk = signal.resample(audio_mono, num_samples)

                rms = np.sqrt(np.mean(resampled_chunk**2))

                # VAD ë¡œì§
                if rms > VAD_THRESHOLD:
                    if not is_speaking:
                        # â”€â”€ Step 3: ë°œí™” ì‹œì‘ ì‹œ í”„ë¦¬ë¡¤ ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì†¡ â”€â”€
                        for pre_chunk in preroll_buf:
                            audio_queue.put(pre_chunk)
                        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    is_speaking = True
                    silence_start = None
                    audio_queue.put(resampled_chunk)
                else:
                    # ì¹¨ë¬µ êµ¬ê°„ì€ í”„ë¦¬ë¡¤ ë²„í¼ì— ë³´ê´€
                    preroll_buf.append(resampled_chunk)
                    if is_speaking:
                        if silence_start is None:
                            silence_start = time.time()

                        audio_queue.put(resampled_chunk)

                        if time.time() - silence_start > SILENCE_TIMEOUT:
                            is_speaking = False
                            silence_start = None
                            audio_queue.put(None)  # ë¬¸ì¥ í™•ì • ì‹ í˜¸

            except IOError: continue
            except KeyboardInterrupt: break

    except KeyboardInterrupt: print("\nì¢…ë£Œ")
    finally:
        stop_event.set()
        if 'stream' in locals(): stream.stop_stream(); stream.close()
        p.terminate()

if __name__ == "__main__":
    main()