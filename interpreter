#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
app_speed_fix.py
- ìµœì í™”: ê³ ì† ë¦¬ìƒ˜í”Œë§(Slicing), í ì ì²´ ë°©ì§€, í™˜ê° ë°©ì§€
"""

import os
import sys
import signal
import queue
import datetime as dt
import numpy as np

print("ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì¤‘...")
try:
    import pyaudiowpatch as pa
    from faster_whisper import WhisperModel
    from deep_translator import GoogleTranslator
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    sys.exit(1)

# =========================================================
# âš¡ ì„¤ì • (ì†ë„ ìµœì í™”)
# =========================================================
# tinyê°€ ê°€ì¥ ë¹ ë¦…ë‹ˆë‹¤. baseë„ ëŠë¦¬ë‹¤ë©´ tinyë¡œ ë°”ê¾¸ì„¸ìš”.
WHISPER_MODEL = "tiny" 
CHUNK_SECONDS = 4.0    # 1.5ì´ˆëŠ” ë¬¸ì¥ì´ ë„ˆë¬´ ì˜ë ¤ì„œ ì˜¤íˆë ¤ ì¸ì‹ì´ ì•ˆë  ìˆ˜ ìˆìŒ (2ì´ˆ ê¶Œì¥)
TARGET_SR = 16000
# =========================================================

# êµ¬ê¸€ ë²ˆì—­ê¸°
translator = GoogleTranslator(source='en', target='ko')

def main():
    global STOP
    signal.signal(signal.SIGINT, lambda s,f: globals().update(STOP=True))

    print(f"ğŸ§  Whisper ëª¨ë¸({WHISPER_MODEL}) ë¡œë”© ì¤‘...")
    # cpu_threadsë¥¼ ë¬¼ë¦¬ ì½”ì–´ ìˆ˜ì— ë§ì¶”ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤ (ë³´í†µ 4~8)
    model = WhisperModel(WHISPER_MODEL, device="cpu", compute_type="int8", cpu_threads=4)
    
    p = pa.PyAudio()
    try:
        # Loopback ì¥ì¹˜ ì°¾ê¸°
        wasapi = p.get_host_api_info_by_type(pa.paWASAPI)
        default_out = p.get_device_info_by_index(wasapi["defaultOutputDevice"])
        
        target = None
        for i in range(p.get_device_count()):
            dev = p.get_device_info_by_index(i)
            if dev["hostApi"] == wasapi["index"] and dev["maxInputChannels"] > 0:
                if dev["name"] == default_out["name"] or "Loopback" in dev["name"]:
                    target = dev
                    break
        if not target: target = p.get_default_input_device_info()
        
        print(f"ğŸ¤ ìº¡ì²˜ ì¥ì¹˜: {target['name']}")
        native_rate = int(target["defaultSampleRate"])
        
        q = queue.Queue()
        def callback(in_data, frame_count, time_info, status):
            q.put(np.frombuffer(in_data, dtype=np.float32))
            return (in_data, pa.paContinue)
            
        stream = p.open(format=pa.paFloat32, channels=target["maxInputChannels"],
                        rate=native_rate, input=True,
                        input_device_index=target["index"], stream_callback=callback)
        
        stream.start_stream()
        print(f"\nğŸš€ ê³ ì† ìë§‰ ëª¨ë“œ ì‹œì‘!")
        
        buf = []
        cur_len = 0
        req_len = int(native_rate * CHUNK_SECONDS)
        
        while not STOP:
            try:
                # íì— ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì´ ìŒ“ì´ë©´(ë ‰ ë°œìƒ ì‹œ) ìµœì‹  ë°ì´í„°ë§Œ ë‚¨ê¸°ê³  ë²„ë¦¼ (Latnecy ë°©ì§€ í•µì‹¬)
                if q.qsize() > 10:
                    print("âš ï¸ ë°ì´í„° ì ì²´ ë°œìƒ! í ì´ˆê¸°í™” (ì‹¤ì‹œê°„ì„± ìœ ì§€)")
                    while not q.empty(): q.get()
                    buf = []
                    cur_len = 0
                    continue

                data = q.get(timeout=0.1)
                
                # ìŠ¤í…Œë ˆì˜¤ -> ëª¨ë…¸ ë³€í™˜ (í‰ê· ê°’ ì‚¬ìš©)
                if target["maxInputChannels"] > 1:
                    data = data.reshape(-1, target["maxInputChannels"]).mean(axis=1)
                
                buf.append(data)
                cur_len += len(data)
                
                if cur_len >= req_len:
                    audio = np.concatenate(buf)
                    buf = []
                    cur_len = 0
                    
                    # [ìµœì í™” 1] ê³ ì† ë¦¬ìƒ˜í”Œë§ (Slicing)
                    # np.interp ëŒ€ì‹  ë°°ì—´ ìŠ¬ë¼ì´ì‹± ì‚¬ìš©. í’ˆì§ˆì€ ì•½ê°„ ë–¨ì–´ì§€ì§€ë§Œ ì†ë„ëŠ” 100ë°° ë¹ ë¦„
                    # ë³´í†µ 48000Hz -> 16000Hz ì´ë¯€ë¡œ 3ì¹¸ì”© ê±´ë„ˆë›°ë©´ ë¨
                    step = int(native_rate / TARGET_SR)
                    if step > 0:
                        audio = audio[::step]
                    
                    # [ìµœì í™” 2] í™˜ê° ë°©ì§€ ì„¤ì • ì¶”ê°€
                    # condition_on_previous_text=False: ì´ì „ ë¬¸ë§¥ ë¬´ì‹œ (ì§§ì€ ë¬¸ì¥ ì¸ì‹ ì†ë„ í–¥ìƒ ë° í™˜ê° ë°©ì§€)
                    segs, _ = model.transcribe(
                        audio, 
                        language="en", 
                        beam_size=1, 
                        vad_filter=True,
                        condition_on_previous_text=False 
                    )
                    
                    text = " ".join([s.text.strip() for s in segs if s.text.strip()])
                    
                    if text:
                        # [ìµœì í™” 3] ë²ˆì—­ ì˜ˆì™¸ ì²˜ë¦¬
                        # ë²ˆì—­ê¸°ê°€ ëŠë ¤ì„œ ë©ˆì¶”ëŠ” ê²ƒì„ ë°©ì§€
                        try:
                            ko = translator.translate(text)
                        except Exception:
                            ko = "(ë²ˆì—­ ì§€ì—°...)"
                        

            except queue.Empty: pass
            
        stream.stop_stream()
        stream.close()
    except Exception as e:
        print(f"ì—ëŸ¬: {e}")
    finally:
        p.terminate()

if __name__ == "__main__":
    main()
